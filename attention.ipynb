{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOvLvwBgRHPuzqGlHYBM4Mx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Song20011219/song/blob/main/attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ji0F2rdrLTNj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\"\"\"\n",
        "Attention blocks\n",
        "Reference: Learn To Pay Attention\n",
        "\"\"\"\n",
        "class ProjectorBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(ProjectorBlock, self).__init__()\n",
        "        self.op = nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
        "            kernel_size=1, padding=0, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.op(x)\n",
        "\n",
        "\n",
        "class ProjectorBlock3D(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(ProjectorBlock3D, self).__init__()\n",
        "        self.op = nn.Conv3d(in_channels=in_channels, out_channels=out_channels,\n",
        "            kernel_size=1, padding=0, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.op(x)\n",
        "\n",
        "\n",
        "class LinearAttentionBlock(nn.Module):\n",
        "    def __init__(self, in_channels, normalize_attn=True):\n",
        "        super(LinearAttentionBlock, self).__init__()\n",
        "        self.normalize_attn = normalize_attn\n",
        "        self.op = nn.Conv2d(in_channels=in_channels, out_channels=1,\n",
        "            kernel_size=1, padding=0, bias=False)\n",
        "\n",
        "    def forward(self, l, g):\n",
        "        N, C, H, W = l.size()\n",
        "        c = self.op(l+g) # (batch_size,1,H,W)\n",
        "        if self.normalize_attn:\n",
        "            a = F.softmax(c.view(N,1,-1), dim=2).view(N,1,H,W)\n",
        "        else:\n",
        "            a = torch.sigmoid(c)\n",
        "        g = torch.mul(a.expand_as(l), l)\n",
        "        if self.normalize_attn:\n",
        "            g = g.view(N,C,-1).sum(dim=2) # (batch_size,C)\n",
        "        else:\n",
        "            g = F.adaptive_avg_pool2d(g, (1,1)).view(N,C)\n",
        "        return c.view(N,1,H,W), g\n",
        "\n",
        "\n",
        "class LinearAttentionBlock3D(nn.Module):\n",
        "    def __init__(self, in_channels, normalize_attn=True):\n",
        "        super(LinearAttentionBlock3D, self).__init__()\n",
        "        self.normalize_attn = normalize_attn\n",
        "        self.op = nn.Conv3d(in_channels=in_channels, out_channels=1,\n",
        "            kernel_size=1, padding=0, bias=False)\n",
        "\n",
        "    def forward(self, l, g):\n",
        "        N, C, T, H, W = l.size()\n",
        "        c = self.op(l+g) # (batch_size,1,T,H,W)\n",
        "        if self.normalize_attn:\n",
        "            a = F.softmax(c.view(N,1,-1), dim=2).view(N,1,T,H,W)\n",
        "        else:\n",
        "            a = torch.sigmoid(c)\n",
        "        g = torch.mul(a.expand_as(l), l)\n",
        "        if self.normalize_attn:\n",
        "            g = g.view(N,C,-1).sum(dim=2) # (batch_size,C)\n",
        "        else:\n",
        "            g = F.adaptive_avg_pool3d(g, (1,1,1)).view(N,C)\n",
        "        return c.view(N,1,T,H,W), g\n",
        "\n",
        "\"\"\"\n",
        "Dense attention block\n",
        "Reference: https://github.com/philipperemy/keras-attention-mechanism\n",
        "\"\"\"\n",
        "class LSTMAttentionBlock(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(LSTMAttentionBlock, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.fc1 = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
        "        self.fc2 = nn.Linear(self.hidden_size*2, self.hidden_size, bias=False)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        # (batch_size, time_steps, hidden_size)\n",
        "        score_first_part = self.fc1(hidden_states)\n",
        "        # (batch_size, hidden_size)\n",
        "        h_t = hidden_states[:,-1,:]\n",
        "        # (batch_size, time_steps)\n",
        "        score = torch.bmm(score_first_part, h_t.unsqueeze(2)).squeeze(2)\n",
        "        attention_weights = F.softmax(score, dim=1)\n",
        "        # (batch_size, hidden_size)\n",
        "        context_vector = torch.bmm(hidden_states.permute(0,2,1), attention_weights.unsqueeze(2)).squeeze(2)\n",
        "        # (batch_size, hidden_size*2)\n",
        "        pre_activation = torch.cat((context_vector, h_t), dim=1)\n",
        "        # (batch_size, hidden_size)\n",
        "        attention_vector = self.fc2(pre_activation)\n",
        "        attention_vector = torch.tanh(attention_vector)\n",
        "\n",
        "        return attention_vector\n",
        "\n",
        "# Test\n",
        "if __name__ == '__main__':\n",
        "    # 2d block\n",
        "    attention_block = LinearAttentionBlock(in_channels=3)\n",
        "    l = torch.randn(16, 3, 128, 128)\n",
        "    g = torch.randn(16, 3, 128, 128)\n",
        "    print(attention_block(l, g))\n",
        "    # 3d block\n",
        "    attention_block_3d = LinearAttentionBlock3D(in_channels=3)\n",
        "    l = torch.randn(16, 3, 16, 128, 128)\n",
        "    g = torch.randn(16, 3, 16, 128, 128)\n",
        "    print(attention_block_3d(l, g))\n",
        "    # LSTM block\n",
        "    attention_block_lstm = LSTMAttentionBlock(hidden_size=256)\n",
        "    hidden_states = torch.randn(32, 16, 256)\n",
        "    print(attention_block_lstm(hidden_states).shape)"
      ]
    }
  ]
}