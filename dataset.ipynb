{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1XVno7xq6HJXtnl2EXnMFWA1VOb_UaQe_",
      "authorship_tag": "ABX9TyN8/WmLskxm5JChjFwehg+Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Song20011219/song/blob/main/dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ie3yOUZSEsEI",
        "outputId": "faaa781a-2a8a-42c3-af3e-2271fece96c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**dataset**"
      ],
      "metadata": {
        "id": "oxjrhZ0bMuyL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "class ASD_Isolated(Dataset):\n",
        "    def __init__(self, data_path, transform=None):\n",
        "        super(ASD_Isolated, self).__init__()\n",
        "        self.data_path = data_path\n",
        "        self.transform = transform\n",
        "        self.frames = 30  # 每个视频样本的帧数\n",
        "        self.data_info = self._get_data_info()\n",
        "\n",
        "    def _get_data_info(self):\n",
        "        data_info = []\n",
        "        for label in (\"arm_flapping\", \"hand_flapping\"):\n",
        "            label_path = os.path.join(self.data_path, label)\n",
        "            for video_folder in os.listdir(label_path):\n",
        "                video_folder_path = os.path.join(label_path, video_folder)\n",
        "                if os.path.isdir(video_folder_path):\n",
        "                    data_info.append((video_folder_path, label))\n",
        "        return data_info\n",
        "\n",
        "    def read_images(self, folder_path):\n",
        "        image_files = sorted([os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith('.jpg')])\n",
        "        assert len(image_files) == self.frames, f\"Expected {self.frames} images, but found {len(image_files)} in folder {folder_path}\"\n",
        "        images = [Image.open(file) for file in image_files]\n",
        "        if self.transform is not None:\n",
        "            images = [self.transform(image) for image in images]\n",
        "        images = torch.stack(images, dim=0)\n",
        "        images = images.permute(1, 0, 2, 3)  # Adjust dimensions for CNN\n",
        "        return images\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_info)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        folder_path, label = self.data_info[idx]\n",
        "        images = self.read_images(folder_path)\n",
        "        label_tensor = torch.tensor(0 if label == \"arm_flapping\" else 1, dtype=torch.long)\n",
        "        return {'data': images, 'label': label_tensor}\n",
        "\n",
        "# 测试\n",
        "if __name__ == '__main__':\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize([128, 128]),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "    ])\n",
        "    dataset = ASD_Isolated(data_path=\"/content/drive/MyDrive/output_frames\", transform=transform)\n",
        "    print(f\"Dataset size: {len(dataset)}\")\n",
        "    sample = dataset[0]\n",
        "    print(f\"Sample image shape: {sample['data'].shape}, Label: {sample['label']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tw65jjqaCK4a",
        "outputId": "754bf32b-1da9-4fcc-e7f0-6d8ee043028d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset size: 163\n",
            "Sample image shape: torch.Size([3, 30, 128, 128]), Label: 0\n"
          ]
        }
      ]
    }
  ]
}