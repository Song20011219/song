{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "https://github.com/Song20011219/song/blob/main/running.ipynb",
      "authorship_tag": "ABX9TyMYIVuXUd34zUc2SKxLvMrN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Song20011219/song/blob/main/running_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ie3yOUZSEsEI",
        "outputId": "85b5c692-f32d-4767-97ce-9f56584bc788"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**dataset**"
      ],
      "metadata": {
        "id": "oxjrhZ0bMuyL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "class ASD_Isolated(Dataset):\n",
        "    def __init__(self, data_path, num_classes=2, transform=None):\n",
        "        super(ASD_Isolated, self).__init__()\n",
        "        self.data_path = data_path  # 数据集根目录\n",
        "        self.num_classes = num_classes  # 数据集中的类别数量\n",
        "        self.transform = transform  # 图像转换操作\n",
        "        self.frames = 30  # 每个视频样本的帧数\n",
        "        self.data_info = self._get_data_info()  # 获取数据信息列表\n",
        "\n",
        "    def _get_data_info(self):\n",
        "        data_info = []\n",
        "        for label in sorted(os.listdir(self.data_path)):  # 假设数据路径下的每个子目录是一个标签类别\n",
        "            label_path = os.path.join(self.data_path, label)  # 标签类别的路径\n",
        "            for video_folder in os.listdir(label_path):\n",
        "                video_folder_path = os.path.join(label_path, video_folder)\n",
        "                if os.path.isdir(video_folder_path):\n",
        "                    data_info.append((video_folder_path, label))  # 添加数据信息（视频文件夹路径和标签）\n",
        "        return data_info\n",
        "\n",
        "    def read_images(self, folder_path):\n",
        "        image_files = sorted([os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.endswith('.jpg')])  # 获取所有帧图像文件路径并排序\n",
        "        assert len(image_files) == self.frames, f\"预期有 {self.frames} 张图像，但在文件夹 {folder_path} 中找到了 {len(image_files)} 张\"\n",
        "        images = [Image.open(file) for file in image_files]  # 读取图像文件并存储为图像对象列表\n",
        "        if self.transform is not None:\n",
        "            images = [self.transform(image) for image in images]  # 如果提供，则应用图像转换操作\n",
        "        images = torch.stack(images, dim=0)  # 将图像列表堆叠为张量\n",
        "        images = images.permute(1, 0, 2, 3)  # 调整维度以适应卷积神经网络\n",
        "        return images\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_info)  # 返回数据集大小（样本数量）\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        folder_path, label = self.data_info[idx]  # 获取指定索引处的数据信息\n",
        "        images = self.read_images(folder_path)  # 读取图像数据\n",
        "        label_index = sorted(os.listdir(self.data_path)).index(label)  # 将标签字符串转换为索引\n",
        "        label_tensor = torch.tensor(label_index, dtype=torch.long)  # 将标签索引转换为张量\n",
        "        return {'data': images, 'label': label_tensor}  # 返回数据和标签的字典\n",
        "\n",
        "# 测试\n",
        "if __name__ == '__main__':\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize([128, 128]),  # 调整图像大小\n",
        "        transforms.ToTensor(),  # 转换为张量\n",
        "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # 标准化\n",
        "    ])\n",
        "    dataset = ASD_Isolated(data_path=\"/content/drive/MyDrive/output_frames\", num_classes=2, transform=transform)  # 创建数据集实例\n",
        "    print(f\"数据集大小: {len(dataset)}\")  # 打印数据集大小\n",
        "    sample = dataset[0]  # 获取第一个样本\n",
        "    print(f\"样本图像形状: {sample['data'].shape}, 标签: {sample['label']}\")  # 打印样本图像形状和标签\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tw65jjqaCK4a",
        "outputId": "b9db3004-0bc1-4e59-af8a-f91e9f8b0047"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "数据集大小: 163\n",
            "样本图像形状: torch.Size([3, 30, 128, 128]), 标签: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**划分训练集测试集**"
      ],
      "metadata": {
        "id": "dbq65QIi0QgS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''import torch\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "# 创建你的完整数据集实例\n",
        "full_dataset = ASD_Isolated(data_path=\"/content/drive/MyDrive/output_frames\", num_classes=2, transform=transform)\n",
        "\n",
        "# 划分数据集\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "test_size = len(full_dataset) - train_size\n",
        "train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])\n",
        "\n",
        "# 创建DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)'''\n"
      ],
      "metadata": {
        "id": "stxh0YPB0SOQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**train**"
      ],
      "metadata": {
        "id": "tbrHOQsoMeJW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Vv932TY3A7XS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def train_epoch(model, criterion, optimizer, dataloader, device, epoch, logger, log_interval, writer):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    all_label = []\n",
        "    all_pred = []\n",
        "\n",
        "    for batch_idx, data in enumerate(dataloader):\n",
        "        # 获取输入和标签\n",
        "        inputs, labels = data['data'].to(device), data['label'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        # 前向传播\n",
        "        outputs = model(inputs)\n",
        "        if isinstance(outputs, list):\n",
        "            outputs = outputs[0]\n",
        "\n",
        "        # 计算损失\n",
        "        loss = criterion(outputs, labels.squeeze())\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        # 计算准确率\n",
        "        prediction = torch.max(outputs, 1)[1]\n",
        "        all_label.extend(labels.squeeze())\n",
        "        all_pred.extend(prediction)\n",
        "        score = accuracy_score(labels.squeeze().cpu().data.squeeze().numpy(), prediction.cpu().data.squeeze().numpy())\n",
        "\n",
        "        # 反向传播 & 优化\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (batch_idx + 1) % log_interval == 0:\n",
        "            logger.info(\"epoch {:3d} | iteration {:5d} | Loss {:.6f} | Acc {:.2f}%\".format(epoch+1, batch_idx+1, loss.item(), score*100))\n",
        "\n",
        "    # 计算平均损失和准确率\n",
        "    training_loss = sum(losses)/len(losses)\n",
        "    all_label = torch.stack(all_label, dim=0)\n",
        "    all_pred = torch.stack(all_pred, dim=0)\n",
        "    training_acc = accuracy_score(all_label.squeeze().cpu().data.squeeze().numpy(), all_pred.cpu().data.squeeze().numpy())\n",
        "    # 记录\n",
        "    writer.add_scalars('Loss', {'train': training_loss}, epoch+1)\n",
        "    writer.add_scalars('Accuracy', {'train': training_acc}, epoch+1)\n",
        "    logger.info(\"第 {} 轮平均训练损失: {:.6f} | 准确率: {:.2f}%\".format(epoch+1, training_loss, training_acc*100))\n",
        "\n",
        "\n",
        "def train_seq2seq(model, criterion, optimizer, clip, dataloader, device, epoch, logger, log_interval, writer):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    all_trg = []\n",
        "    all_pred = []\n",
        "    all_wer = []\n",
        "\n",
        "    for batch_idx, (imgs, target) in enumerate(dataloader):\n",
        "        imgs = imgs.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        # 前向传播\n",
        "        outputs = model(imgs, target)\n",
        "\n",
        "        # 目标：(batch_size, trg长度)\n",
        "        # outputs：(trg长度, batch_size, 输出维度)\n",
        "        # 跳过sos\n",
        "        output_dim = outputs.shape[-1]\n",
        "        outputs = outputs[1:].view(-1, output_dim)\n",
        "        target = target.permute(1,0)[1:].reshape(-1)\n",
        "\n",
        "        # 计算损失\n",
        "        loss = criterion(outputs, target)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        # 计算准确率\n",
        "        prediction = torch.max(outputs, 1)[1]\n",
        "        score = accuracy_score(target.cpu().data.squeeze().numpy(), prediction.cpu().data.squeeze().numpy())\n",
        "        all_trg.extend(target)\n",
        "        all_pred.extend(prediction)\n",
        "\n",
        "        # 计算WER\n",
        "        # prediction: ((trg长度-1)*batch_size)\n",
        "        # target: ((trg长度-1)*batch_size)\n",
        "        batch_size = imgs.shape[0]\n",
        "        prediction = prediction.view(-1, batch_size).permute(1,0).tolist()\n",
        "        target = target.view(-1, batch_size).permute(1,0).tolist()\n",
        "        wers = []\n",
        "        for i in range(batch_size):\n",
        "            # 添加掩码（去除填充、sos、eos）\n",
        "            prediction[i] = [item for item in prediction[i] if item not in [0,1,2]]\n",
        "            target[i] = [item for item in target[i] if item not in [0,1,2]]\n",
        "            wers.append(wer(target[i], prediction[i]))\n",
        "        all_wer.extend(wers)\n",
        "\n",
        "        # 反向传播 & 优化\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        if (batch_idx + 1) % log_interval == 0:\n",
        "            logger.info(\"第 {} 轮 | 迭代 {:5d} | 损失 {:.6f} | 准确率 {:.2f}% | WER {:.2f}%\".format(epoch+1, batch_idx+1, loss.item(), score*100, sum(wers)/len(wers)))\n",
        "\n",
        "    # 计算平均损失、准确率和WER\n",
        "    training_loss = sum(losses)/len(losses)\n",
        "    all_trg = torch.stack(all_trg, dim=0)\n",
        "    all_pred = torch.stack(all_pred, dim=0)\n",
        "    training_acc = accuracy_score(all_trg.cpu().data.squeeze().numpy(), all_pred.cpu().data.squeeze().numpy())\n",
        "    training_wer = sum(all_wer)/len(all_wer)\n",
        "    # 记录\n",
        "    writer.add_scalars('Loss', {'train': training_loss}, epoch+1)\n",
        "    writer.add_scalars('Accuracy', {'train': training_acc}, epoch+1)\n",
        "    writer.add_scalars('WER', {'train': training_wer}, epoch+1)\n",
        "    logger.info(\"第 {} 轮平均训练损失: {:.6f} | 准确率: {:.2f}% | WER {:.2f}%\".format(epoch+1, training_loss, training_acc*100,training_wer))\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**tool**"
      ],
      "metadata": {
        "id": "NPCKwNCvNA3s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7s4Fqh9I-Zhy",
        "outputId": "82b03302-16ca-42b8-b129-255685b2ff63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "75.0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torchvision.utils as utils\n",
        "import cv2\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "def get_label_and_pred(model, dataloader, device):\n",
        "    all_label = []\n",
        "    all_pred = []\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, data in enumerate(test_loader):\n",
        "            # get the inputs and labels\n",
        "            inputs, labels = data['data'].to(device), data['label'].to(device)\n",
        "            # forward\n",
        "            outputs = model(inputs)\n",
        "            if isinstance(outputs, list):\n",
        "                outputs = outputs[0]\n",
        "            # collect labels & prediction\n",
        "            prediction = torch.max(outputs, 1)[1]\n",
        "            all_label.extend(labels.squeeze())\n",
        "            all_pred.extend(prediction)\n",
        "    # Compute accuracy\n",
        "    all_label = torch.stack(all_label, dim=0)\n",
        "    all_pred = torch.stack(all_pred, dim=0)\n",
        "    all_label = all_label.squeeze().cpu().data.squeeze().numpy()\n",
        "    all_pred = all_pred.cpu().data.squeeze().numpy()\n",
        "    return all_label, all_pred\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(model, dataloader, device, save_path='confmat.png', normalize=True):\n",
        "    # Get prediction\n",
        "    all_label, all_pred = get_label_and_pred(model, dataloader, device)\n",
        "    confmat = confusion_matrix(all_label, all_pred)\n",
        "\n",
        "    # Normalize the matrix\n",
        "    if normalize:\n",
        "        confmat = confmat.astype('float') / confmat.sum(axis=1)[:, np.newaxis]\n",
        "    # Draw matrix\n",
        "    plt.figure(figsize=(20,20))\n",
        "    # confmat = np.random.rand(100,100)\n",
        "    plt.imshow(confmat, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "    plt.colorbar()\n",
        "    # Add ticks\n",
        "    ticks = np.arange(100)\n",
        "    plt.xticks(ticks, fontsize=8)\n",
        "    plt.yticks(ticks, fontsize=8)\n",
        "    plt.grid(True)\n",
        "    # Add title & labels\n",
        "    plt.title('Confusion matrix', fontsize=20)\n",
        "    plt.xlabel('Predicted label', fontsize=20)\n",
        "    plt.ylabel('True label', fontsize=20)\n",
        "    # Save figure\n",
        "    plt.savefig(save_path)\n",
        "\n",
        "    # Ranking\n",
        "    sorted_index = np.diag(confmat).argsort()\n",
        "    for i in range(10):\n",
        "        # print(type(sorted_index[i]))\n",
        "        print(test_set.label_to_word(int(sorted_index[i])), confmat[sorted_index[i]][sorted_index[i]])\n",
        "    # Save to csv\n",
        "    np.savetxt('matrix.csv', confmat, delimiter=',')\n",
        "\n",
        "\n",
        "def visualize_attn(I, c):\n",
        "    # Image\n",
        "    img = I.permute((1,2,0)).cpu().numpy()\n",
        "    # Heatmap\n",
        "    N, C, H, W = c.size()\n",
        "    a = F.softmax(c.view(N,C,-1), dim=2).view(N,C,H,W)\n",
        "    up_factor = 128/H\n",
        "    # print(up_factor, I.size(), c.size())\n",
        "    if up_factor > 1:\n",
        "        a = F.interpolate(a, scale_factor=up_factor, mode='bilinear', align_corners=False)\n",
        "    attn = utils.make_grid(a, nrow=4, normalize=True, scale_each=True)\n",
        "    attn = attn.permute((1,2,0)).mul(255).byte().cpu().numpy()\n",
        "    attn = cv2.applyColorMap(attn, cv2.COLORMAP_JET)\n",
        "    attn = cv2.cvtColor(attn, cv2.COLOR_BGR2RGB)\n",
        "    # Add the heatmap to the image\n",
        "    vis = 0.6 * img + 0.4 * attn\n",
        "    return torch.from_numpy(vis).permute(2,0,1)\n",
        "\n",
        "\n",
        "def plot_attention_map(model, dataloader, device):\n",
        "    # Summary writer\n",
        "    writer = SummaryWriter(\"runs/attention_{:%Y-%m-%d_%H-%M-%S}\".format(datetime.now()))\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, data in enumerate(dataloader):\n",
        "            # get images\n",
        "            inputs = data['data'].to(device)\n",
        "            if batch_idx == 0:\n",
        "                images = inputs[0:16,:,:,:,:]\n",
        "                I = utils.make_grid(images[:,:,0,:,:], nrow=4, normalize=True, scale_each=True)\n",
        "                writer.add_image('origin', I)\n",
        "                _, c1, c2, c3, c4 = model(images)\n",
        "                # print(I.shape, c1.shape, c2.shape, c3.shape, c4.shape)\n",
        "                attn1 = visualize_attn(I, c1[:,:,0,:,:])\n",
        "                writer.add_image('attn1', attn1)\n",
        "                attn2 = visualize_attn(I, c2[:,:,0,:,:])\n",
        "                writer.add_image('attn2', attn2)\n",
        "                attn3 = visualize_attn(I, c3[:,:,0,:,:])\n",
        "                writer.add_image('attn3', attn3)\n",
        "                attn4 = visualize_attn(I, c4[:,:,0,:,:])\n",
        "                writer.add_image('attn4', attn4)\n",
        "                break\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Calculate Word Error Rate\n",
        "Word Error Rate = (Substitutions + Insertions + Deletions) / Number of Words Spoken\n",
        "Reference:\n",
        "https://holianh.github.io/portfolio/Cach-tinh-WER/\n",
        "https://github.com/imalic3/python-word-error-rate\n",
        "\"\"\"\n",
        "def wer(r, h):\n",
        "    # initialisation\n",
        "    d = np.zeros((len(r)+1)*(len(h)+1), dtype=np.uint8)\n",
        "    d = d.reshape((len(r)+1, len(h)+1))\n",
        "    for i in range(len(r)+1):\n",
        "        for j in range(len(h)+1):\n",
        "            if i == 0:\n",
        "                d[0][j] = j\n",
        "            elif j == 0:\n",
        "                d[i][0] = i\n",
        "\n",
        "    # computation\n",
        "    for i in range(1, len(r)+1):\n",
        "        for j in range(1, len(h)+1):\n",
        "            if r[i-1] == h[j-1]:\n",
        "                d[i][j] = d[i-1][j-1]\n",
        "            else:\n",
        "                substitution = d[i-1][j-1] + 1\n",
        "                insertion = d[i][j-1] + 1\n",
        "                deletion = d[i-1][j] + 1\n",
        "                d[i][j] = min(substitution, insertion, deletion)\n",
        "\n",
        "    return float(d[len(r)][len(h)]) / len(r) * 100\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Calculate WER\n",
        "    r = [1,2,3,4]\n",
        "    h = [1,1,3,5,6]\n",
        "    print(wer(r, h))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**validation**"
      ],
      "metadata": {
        "id": "DSu2jA0INbkE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "IUjm0jvkCHMG"
      },
      "outputs": [],
      "source": [
        "def val_epoch(model, criterion, dataloader, device, epoch, logger, writer):\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, data in enumerate(dataloader):\n",
        "            inputs = data['data'].to(device)\n",
        "            labels = data['label'].to(device)\n",
        "\n",
        "            if not isinstance(inputs, torch.Tensor) or not isinstance(labels, torch.Tensor):\n",
        "                print(f'Error: Data or labels are not tensors at batch index {batch_idx}.')\n",
        "                continue\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            if isinstance(outputs, tuple) or isinstance(outputs, list):\n",
        "                outputs = outputs[0]\n",
        "\n",
        "            if outputs.size(0) != labels.size(0):\n",
        "                print(f'Error: Mismatch between output size {outputs.size(0)} and labels size {labels.size(0)} at batch index {batch_idx}.')\n",
        "                continue\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            all_labels.append(labels)\n",
        "            all_preds.append(predicted)\n",
        "\n",
        "    validation_loss = sum(losses) / len(losses)\n",
        "    all_labels = torch.cat(all_labels).cpu()\n",
        "    all_preds = torch.cat(all_preds).cpu()\n",
        "    validation_acc = accuracy_score(all_labels.numpy(), all_preds.numpy())\n",
        "\n",
        "    writer.add_scalar('Loss/val', validation_loss, epoch)\n",
        "    writer.add_scalar('Accuracy/val', validation_acc, epoch)\n",
        "    logger.info(f'Validation - Epoch: {epoch}, Loss: {validation_loss:.4f}, Accuracy: {validation_acc:.4f}')\n",
        "\n",
        "    return validation_loss, validation_acc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "conv3d"
      ],
      "metadata": {
        "id": "Vs8zXYZtNk3f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.hub import load_state_dict_from_url\n",
        "import torchvision\n",
        "from functools import partial\n",
        "from collections import OrderedDict\n",
        "import math\n",
        "\n",
        "import os,inspect,sys\n",
        "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
        "sys.path.insert(0,currentdir)\n",
        "#from Attention import ProjectorBlock3D, LinearAttentionBlock3D\n",
        "\n",
        "\"\"\"\n",
        "Implementation of 3D CNN.\n",
        "\"\"\"\n",
        "class CNN3D(nn.Module):\n",
        "    def __init__(self, sample_size=128, sample_duration=16, drop_p=0.0, hidden1=512, hidden2=256, num_classes=100):\n",
        "        super(CNN3D, self).__init__()\n",
        "        self.sample_size = sample_size\n",
        "        self.sample_duration = sample_duration\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # network params\n",
        "        self.ch1, self.ch2, self.ch3 = 32, 48, 48\n",
        "        self.k1, self.k2, self.k3 = (3,7,7), (3,7,7), (3,5,5)\n",
        "        self.s1, self.s2, self.s3 = (2,2,2), (2,2,2), (2,2,2)\n",
        "        self.p1, self.p2, self.p3 = (0,0,0), (0,0,0), (0,0,0)\n",
        "        self.d1, self.d2, self.d3 = (1,1,1), (1,1,1), (1,1,1)\n",
        "        self.hidden1, self.hidden2 = hidden1, hidden2\n",
        "        self.drop_p = drop_p\n",
        "        self.pool_k, self.pool_s, self.pool_p, self.pool_d = (1,2,2), (1,2,2), (0,0,0), (1,1,1)\n",
        "        # Conv1\n",
        "        self.conv1_output_shape = self.compute_output_shape(self.sample_duration, self.sample_size,\n",
        "            self.sample_size, self.k1, self.s1, self.p1, self.d1)\n",
        "        # self.conv1_output_shape = self.compute_output_shape(self.conv1_output_shape[0], self.conv1_output_shape[1],\n",
        "        #     self.conv1_output_shape[2], self.pool_k, self.pool_s, self.pool_p, self.pool_d)\n",
        "        # Conv2\n",
        "        self.conv2_output_shape = self.compute_output_shape(self.conv1_output_shape[0], self.conv1_output_shape[1],\n",
        "            self.conv1_output_shape[2], self.k2, self.s2, self.p2, self.d2)\n",
        "        # self.conv2_output_shape = self.compute_output_shape(self.conv2_output_shape[0], self.conv2_output_shape[1],\n",
        "        #     self.conv2_output_shape[2], self.pool_k, self.pool_s, self.pool_p, self.pool_d)\n",
        "        # Conv3\n",
        "        self.conv3_output_shape = self.compute_output_shape(self.conv2_output_shape[0], self.conv2_output_shape[1],\n",
        "            self.conv2_output_shape[2], self.k3, self.s3, self.p3, self.d3)\n",
        "        # print(self.conv1_output_shape, self.conv2_output_shape, self.conv3_output_shape)\n",
        "\n",
        "        # network architecture\n",
        "        # in_channels=1 for grayscale, 3 for rgb\n",
        "        self.conv1 = nn.Conv3d(in_channels=3, out_channels=self.ch1, kernel_size=self.k1,\n",
        "            stride=self.s1, padding=self.p1, dilation=self.d1)\n",
        "        self.bn1 = nn.BatchNorm3d(self.ch1)\n",
        "        self.conv2 = nn.Conv3d(in_channels=self.ch1, out_channels=self.ch2, kernel_size=self.k2,\n",
        "            stride=self.s2, padding=self.p2, dilation=self.d2)\n",
        "        self.bn2 = nn.BatchNorm3d(self.ch2)\n",
        "        self.conv3 = nn.Conv3d(in_channels=self.ch2, out_channels=self.ch3, kernel_size=self.k3,\n",
        "            stride=self.s3, padding=self.p3, dilation=self.d3)\n",
        "        self.bn3 = nn.BatchNorm3d(self.ch3)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.drop = nn.Dropout3d(p=self.drop_p)\n",
        "        self.pool = nn.MaxPool3d(kernel_size=self.pool_k)\n",
        "        self.fc1 = nn.Linear(self.ch3 * self.conv3_output_shape[0] * self.conv3_output_shape[1] * self.conv3_output_shape[2], self.hidden1)\n",
        "        self.fc2 = nn.Linear(self.hidden1, self.hidden2)\n",
        "        self.fc3 = nn.Linear(self.hidden2, self.num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Conv1\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        # x = self.pool(x)\n",
        "        # x = self.drop(x)\n",
        "        # Conv2\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "        # x = self.pool(x)\n",
        "        # x = self.drop(x)\n",
        "        # Conv3\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x)\n",
        "        x = self.relu(x)\n",
        "        # x = self.drop(x)\n",
        "        # MLP\n",
        "        # print(x.shape)\n",
        "        # x.size(0) ------ batch_size\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.dropout(x, p=self.drop_p, training=self.training)\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def compute_output_shape(self, D_in, H_in, W_in, k, s, p, d):\n",
        "        # Conv\n",
        "        D_out = np.floor((D_in + 2*p[0] - d[0]*(k[0] - 1) - 1)/s[0] + 1).astype(int)\n",
        "        H_out = np.floor((H_in + 2*p[1] - d[1]*(k[1] - 1) - 1)/s[1] + 1).astype(int)\n",
        "        W_out = np.floor((W_in + 2*p[2] - d[2]*(k[2] - 1) - 1)/s[2] + 1).astype(int)\n",
        "\n",
        "        return D_out, H_out, W_out\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Implementation of 3D Resnet\n",
        "Reference: Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet?\n",
        "\"\"\"\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "    # planes refer to the number of feature maps\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.stride = stride\n",
        "        self.downsample = downsample\n",
        "        self.conv1 = nn.Conv3d(\n",
        "            inplanes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm3d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv3d(\n",
        "            planes, planes, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm3d(planes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        # conv1\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        # conv2\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        # downsample\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        # print(out.shape, residual.shape)\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "    # planes refer to the number of feature maps\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.stride = stride\n",
        "        self.downsample = downsample\n",
        "        self.conv1 = nn.Conv3d(\n",
        "            inplanes, planes, kernel_size=1, bias=False) # kernal_size=1 don't need padding\n",
        "        self.bn1 = nn.BatchNorm3d(planes)\n",
        "        self.conv2 = nn.Conv3d(\n",
        "            planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm3d(planes)\n",
        "        self.conv3 = nn.Conv3d(planes, planes * 4, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm3d(planes * 4)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        # conv1\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        # conv2\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "        # conv3\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "        # downsample\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        # print(out.shape, residual.shape)\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "def downsample_basic_block(x, planes, stride):\n",
        "    # decrease data resolution if stride not equals to 1\n",
        "    out = F.avg_pool3d(x, kernel_size=1, stride=stride)\n",
        "    # shape: (batch_size, channel, t, h, w)\n",
        "    # try to match the channel size\n",
        "    zero_pads = torch.Tensor(\n",
        "        out.size(0), planes - out.size(1), out.size(2), out.size(3),\n",
        "        out.size(4)).zero_()\n",
        "    if isinstance(out.data, torch.cuda.FloatTensor):\n",
        "        zero_pads = zero_pads.cuda()\n",
        "\n",
        "    out = Variable(torch.cat([out.data, zero_pads], dim=1))\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, layers, shortcut_type, sample_size, sample_duration, attention=False, num_classes=500):\n",
        "        super(ResNet, self).__init__()\n",
        "        # initialize inplanes to 64, it'll be changed later\n",
        "        self.inplanes = 64\n",
        "        self.conv1 = nn.Conv3d(\n",
        "            3, 64, kernel_size=7, stride=(1, 2, 2), padding=(3, 3, 3), bias=False)\n",
        "        self.bn1 = nn.BatchNorm3d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool3d(kernel_size=(3, 3, 3), stride=2, padding=1)\n",
        "        # layers refers to the number of blocks in each layer\n",
        "        self.layer1 = self._make_layer(\n",
        "            block, 64, layers[0], shortcut_type, stride=1)\n",
        "        self.layer2 = self._make_layer(\n",
        "            block, 128, layers[1], shortcut_type, stride=2)\n",
        "        self.layer3 = self._make_layer(\n",
        "            block, 256, layers[2], shortcut_type, stride=2)\n",
        "        self.layer4 = self._make_layer(\n",
        "            block, 512, layers[3], shortcut_type, stride=2)\n",
        "        # calclatue kernal size for average pooling\n",
        "        last_duration = int(math.ceil(sample_duration / 16))\n",
        "        last_size = int(math.ceil(sample_size / 32))\n",
        "        self.avgpool = nn.AvgPool3d(\n",
        "            (last_duration, last_size, last_size), stride=1)\n",
        "        # attention blocks\n",
        "        self.attention = attention\n",
        "        if self.attention:\n",
        "            self.attn1 = LinearAttentionBlock3D(in_channels=512*block.expansion, normalize_attn=True)\n",
        "            self.attn2 = LinearAttentionBlock3D(in_channels=512*block.expansion, normalize_attn=True)\n",
        "            self.attn3 = LinearAttentionBlock3D(in_channels=512*block.expansion, normalize_attn=True)\n",
        "            self.attn4 = LinearAttentionBlock3D(in_channels=512*block.expansion, normalize_attn=True)\n",
        "            self.projector1 = ProjectorBlock3D(in_channels=64*block.expansion, out_channels=512*block.expansion)\n",
        "            self.projector2 = ProjectorBlock3D(in_channels=128*block.expansion, out_channels=512*block.expansion)\n",
        "            self.projector3 = ProjectorBlock3D(in_channels=256*block.expansion, out_channels=512*block.expansion)\n",
        "            self.fc = nn.Linear(512 * block.expansion * 4, num_classes)\n",
        "        else:\n",
        "            self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "        # init the weights\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv3d):\n",
        "                m.weight = nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
        "            elif isinstance(m, nn.BatchNorm3d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, shortcut_type, stride):\n",
        "        downsample = None\n",
        "        # when the in-channel and the out-channel dismatch, downsample!!!\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            # stride once for downsample and block.\n",
        "            if shortcut_type == 'A':\n",
        "                downsample = partial(\n",
        "                    downsample_basic_block,\n",
        "                    planes=planes * block.expansion,\n",
        "                    stride=stride)\n",
        "            else:\n",
        "                downsample = nn.Sequential(\n",
        "                    nn.Conv3d(\n",
        "                        self.inplanes,\n",
        "                        planes * block.expansion,\n",
        "                        kernel_size=1,\n",
        "                        stride=stride,\n",
        "                        bias=False), nn.BatchNorm3d(planes * block.expansion))\n",
        "\n",
        "        layers = []\n",
        "        # only the first block needs downsample.\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        # change inplanes for the next layer\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        l1 = self.layer1(x)\n",
        "        l2 = self.layer2(l1)\n",
        "        l3 = self.layer3(l2)\n",
        "        l4 = self.layer4(l3)\n",
        "\n",
        "        g = self.avgpool(l4)\n",
        "        # attention\n",
        "        if self.attention:\n",
        "            # print(l1.shape, l2.shape, l3.shape, l4.shape, g.shape)\n",
        "            c1, g1 = self.attn1(self.projector1(l1), g)\n",
        "            c2, g2 = self.attn2(self.projector2(l2), g)\n",
        "            c3, g3 = self.attn3(self.projector3(l3), g)\n",
        "            c4, g4 = self.attn4(l4, g)\n",
        "            g = torch.cat((g1,g2,g3,g4), dim=1)\n",
        "            x = self.fc(g)\n",
        "        else:\n",
        "            c1, c2, c3, c4 = None, None, None, None\n",
        "            # x.size(0) ------ batch_size\n",
        "            g = g.view(g.size(0), -1)\n",
        "            x = self.fc(g)\n",
        "\n",
        "        return [x, c1, c2, c3, c4]\n",
        "\n",
        "    def load_my_state_dict(self, state_dict):\n",
        "        my_state_dict = self.state_dict()\n",
        "        for name, param in state_dict.items():\n",
        "            if name == 'fc.weight' or name == 'fc.bias':\n",
        "                continue\n",
        "            my_state_dict[name].copy_(param.data)\n",
        "\n",
        "\n",
        "import torchvision.models as models\n",
        "\n",
        "# ...[省略其他import和类定义]...\n",
        "\n",
        "def resnet18(pretrained=False, progress=True, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-18 model.\"\"\"\n",
        "    model = models.resnet18(pretrained=pretrained, progress=progress, **kwargs)\n",
        "    return model\n",
        "\n",
        "def resnet34(pretrained=False, progress=True, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-34 model.\"\"\"\n",
        "    model = models.resnet34(pretrained=pretrained, progress=progress, **kwargs)\n",
        "    return model\n",
        "\n",
        "def resnet101(pretrained=False, progress=True, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-101 model.\"\"\"\n",
        "    model = models.resnet101(pretrained=pretrained, progress=progress, **kwargs)\n",
        "    return model\n",
        "\n",
        "def resnet50(pretrained=False, progress=True, **kwargs):\n",
        "    \"\"\"构建一个ResNet-50模型。\"\"\"\n",
        "    model = models.resnet50(pretrained=pretrained,progress=progress, **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet152(pretrained=False, progress=True, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-152 model.\"\"\"\n",
        "    model = models.resnet152(pretrained=pretrained, progress=progress, **kwargs)\n",
        "    return model\n",
        "\n",
        "def resnet200(pretrained=False, progress=True, **kwargs):\n",
        "    \"\"\"Constructs a ResNet-200 model.\"\"\"\n",
        "    model = models.resnet200(pretrained=pretrained, progress=progress, **kwargs)\n",
        "    return model\n",
        "\n",
        "\n",
        "# ...[省略其他函数和类定义]...\n",
        "\n",
        "# 使用时可以直接调用\n",
        "#cnn3d = resnet50(pretrained=True, progress=True, num_classes=num_classes)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "3D CNN Models from torchvision.models\n",
        "Reference: https://pytorch.org/docs/stable/torchvision/models.html#video-classification\n",
        "\"\"\"\n",
        "class r3d_18(nn.Module):\n",
        "    def __init__(self, pretrained=True, num_classes=500):\n",
        "        super(r3d_18, self).__init__()\n",
        "        self.pretrained = pretrained\n",
        "        self.num_classes = num_classes\n",
        "        model = torchvision.models.video.r3d_18(pretrained=self.pretrained)\n",
        "        # delete the last fc layer\n",
        "        modules = list(model.children())[:-1]\n",
        "        # print(modules)\n",
        "        self.r3d_18 = nn.Sequential(*modules)\n",
        "        self.fc1 = nn.Linear(model.fc.in_features, self.num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.r3d_18(x)\n",
        "        # print(out.shape)\n",
        "        # Flatten the layer to fc\n",
        "        out = out.flatten(1)\n",
        "        out = self.fc1(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class mc3_18(nn.Module):\n",
        "    def __init__(self, pretrained=True, num_classes=500):\n",
        "        super(mc3_18, self).__init__()\n",
        "        self.pretrained = pretrained\n",
        "        self.num_classes = num_classes\n",
        "        model = torchvision.models.video.mc3_18(pretrained=self.pretrained)\n",
        "        # delete the last fc layer\n",
        "        modules = list(model.children())[:-1]\n",
        "        # print(modules)\n",
        "        self.mc3_18 = nn.Sequential(*modules)\n",
        "        self.fc1 = nn.Linear(model.fc.in_features, self.num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.mc3_18(x)\n",
        "        # print(out.shape)\n",
        "        # Flatten the layer to fc\n",
        "        out = out.flatten(1)\n",
        "        out = self.fc1(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class r2plus1d_18(nn.Module):\n",
        "    def __init__(self, pretrained=True, num_classes=500):\n",
        "        super(r2plus1d_18, self).__init__()\n",
        "        self.pretrained = pretrained\n",
        "        self.num_classes = num_classes\n",
        "        model = torchvision.models.video.r2plus1d_18(pretrained=self.pretrained)\n",
        "        # delete the last fc layer\n",
        "        modules = list(model.children())[:-1]\n",
        "        # print(modules)\n",
        "        self.r2plus1d_18 = nn.Sequential(*modules)\n",
        "        self.fc1 = nn.Linear(model.fc.in_features, self.num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.r2plus1d_18(x)\n",
        "        # print(out.shape)\n",
        "        # Flatten the layer to fc\n",
        "        out = out.flatten(1)\n",
        "        out = self.fc1(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "# 调整数据维度以匹配模型的输入要求\n",
        "#data = dataset[0]['data'].permute(1, 0, 2, 3, 4)  # 将维度重排为 [num_channels, 1, num_frames, height, width]\n",
        "\n",
        "# 选择你希望使用的帧数\n",
        "#desired_num_frames = 30  # 选择你希望使用的帧数\n",
        "#data = data[:, :, :desired_num_frames, :, :]  # 切片选择帧数\n",
        "#data = data.unsqueeze(0)  # 添加批次维度，变成 [1, num_channels, num_frames, height, width]\n",
        "\n",
        "# 将处理后的数据传递给模型进行推理\n",
        "#output = cnn3d(data)\n",
        "\n",
        "# 在这里可以处理模型的输出，如计算损失或进行后续的操作\n",
        "\n",
        "\n",
        "# Test\n",
        "if __name__ == '__main__':\n",
        "    import sys\n",
        "    sys.path.append(\"..\")\n",
        "    import torchvision.transforms as transforms\n",
        "\n",
        "    sample_size = 128\n",
        "    sample_duration = 16\n",
        "    num_classes = 2\n",
        "    transform = transforms.Compose([transforms.Resize([sample_size, sample_size]), transforms.ToTensor()])\n",
        "    dataset = ASD_Isolated(data_path=\"/content/drive/MyDrive/output_frames\",\n",
        "\n",
        "        num_classes=num_classes, transform=transform)\n",
        "    #cnn3d = CNN3D(sample_size=sample_size, sample_duration=sample_duration, num_classes=num_classes)\n",
        "    #cnn3d = resnet50(pretrained=True, progress=True, sample_size=sample_size, sample_duration=sample_duration, attention=True, num_classes=num_classes)\n",
        "    cnn3d = r3d_18(pretrained=True, num_classes=num_classes)\n",
        "    # cnn3d = mc3_18(pretrained=True, num_classes=num_classes)\n",
        "    # cnn3d = r2plus1d_18(pretrained=True, num_classes=num_classes)\n",
        "    # print(dataset[0]['images'].shape)\n",
        "\n",
        "\n",
        "\n",
        "    print(cnn3d(dataset[0]['data'].unsqueeze(0)))\n",
        "\n",
        "    # Test for loading pretrained models\n",
        "    # state_dict = torch.load('resnet-18-kinetics.pth')\n",
        "    # for name, param in state_dict.items():\n",
        "    #     print(name)\n",
        "    # # print(state_dict['arch'])\n",
        "    # # print(state_dict['optimizer'])\n",
        "    # # print(state_dict['epoch'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cc53b38-a068-485e-c7d7-0ae5435c165d",
        "id": "ppsYw5NDNv24"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=R3D_18_Weights.KINETICS400_V1`. You can also use `weights=R3D_18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/r3d_18-b3b3357e.pth\" to /root/.cache/torch/hub/checkpoints/r3d_18-b3b3357e.pth\n",
            "100%|██████████| 127M/127M [00:03<00:00, 41.1MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.0671, -0.1974]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Training set size: {train_size}\")\n",
        "print(f\"Validation set size: {val_size}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PMGItDtQW08",
        "outputId": "ba1f67c2-081f-4437-b8a6-237a5fb2a8fa"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 130\n",
            "Validation set size: 33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (data, target) in enumerate(val_loader):\n",
        "    if i == 1:  # Just check the first batch\n",
        "        print(data.shape, target.shape)\n",
        "        break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "TRGmPkT-QZEa",
        "outputId": "c6cd789b-01f9-4f46-cec0-eb78c0e2070f"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-641efd6d8b01>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Just check the first batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'shape'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ASD_CNN3D**"
      ],
      "metadata": {
        "id": "2UmhxQsr3roB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "import logging\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torchvision.transforms as transforms\n",
        "#from dataset import ASD_Isol\n",
        "#from train import train_epoch\n",
        "#from validation import val_epoch\n",
        "#from Conv3D import CNN3D  # 导入Conv3D模型\n",
        "\n",
        "# 设置路径\n",
        "data_path = \"/content/drive/MyDrive/output_frames\"\n",
        "model_path = \"/content/drive/MyDrive/cnn3d_models\"\n",
        "log_path = \"cnn3d_log_{:%Y-%m-%d_%H-%M-%S}.log\".format(datetime.now())\n",
        "sum_path = \"cnn3d_runs_{:%Y-%m-%d_%H-%M-%S}\".format(datetime.now())\n",
        "\n",
        "# 记录到文件和Tensorboard\n",
        "logging.basicConfig(level=logging.INFO, format='%(message)s', handlers=[logging.FileHandler(log_path), logging.StreamHandler()])\n",
        "logger = logging.getLogger('CNN3D')\n",
        "logger.info('记录到文件...')\n",
        "writer = SummaryWriter(sum_path)\n",
        "\n",
        "# 设备设置\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 超参数\n",
        "num_classes = 2\n",
        "epochs = 10\n",
        "batch_size = 8\n",
        "learning_rate = 1e-4\n",
        "log_interval = 20\n",
        "sample_size = 128\n",
        "sample_duration = 30  # 30帧的视频样本\n",
        "\n",
        "# 加载数据\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize([sample_size, sample_size]),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "dataset = ASD_Isolated(data_path=data_path, transform=transform)\n",
        "total_samples = len(dataset)\n",
        "train_size = int(0.8 * total_samples)\n",
        "val_size = total_samples - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "# 创建Conv3D模型\n",
        "model = CNN3D(sample_size=sample_size, sample_duration=sample_duration, num_classes=num_classes).to(device)\n",
        "\n",
        "# 创建损失函数和优化器\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# 开始训练\n",
        "logger.info(\"开始训练\".center(60, '#'))\n",
        "for epoch in range(epochs):\n",
        "    # 训练模型\n",
        "    train_epoch(model, criterion, optimizer, train_loader, device, epoch, logger, log_interval, writer)\n",
        "\n",
        "    # 验证模型\n",
        "    val_epoch(model, criterion, val_loader, device, epoch, logger, writer)\n",
        "\n",
        "    # 保存模型\n",
        "    torch.save(model.state_dict(), os.path.join(model_path, \"cnn3d_epoch{:03d}.pth\".format(epoch+1)))\n",
        "    logger.info(\"第 {} 轮模型已保存\".format(epoch+1).center(60, '#'))\n",
        "\n",
        "logger.info(\"训练完成\".center(60, '#'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10FKEslpMpx9",
        "outputId": "8a7e7282-d850-4992-bae3-321bfa136132"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 加载模型\n",
        "model_path = '/content/drive/MyDrive/cnn3d_models/cnn3d_epoch001.pth'\n",
        "model = CNN3D(sample_size=sample_size, sample_duration=sample_duration, num_classes=num_classes).to(device)\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "# 设置模型为评估模式\n",
        "model.eval()\n",
        "\n",
        "# 创建测试数据加载器\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "# 初始化性能指标\n",
        "total_correct = 0\n",
        "total_samples = 0\n",
        "\n",
        "# 遍历测试数据集\n",
        "for data, labels in test_loader:\n",
        "    data, labels = data.to(device), labels.to(device)\n",
        "\n",
        "    # 进行推理\n",
        "    with torch.no_grad():  # 禁用梯度计算\n",
        "        outputs = model(data)\n",
        "\n",
        "    # 计算准确性\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    total_samples += labels.size(0)\n",
        "    total_correct += (predicted == labels).sum().item()\n",
        "\n",
        "# 计算准确性\n",
        "accuracy = total_correct / total_samples\n",
        "print(\"测试集准确性: {:.2f}%\".format(accuracy * 100))\n"
      ],
      "metadata": {
        "id": "5drvkC3LyhxX",
        "outputId": "fd3864b1-c1bf-4694-e1f3-69a77010033f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-6e304ed2eb0c>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# 遍历测试数据集\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# 进行推理\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'to'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**test**"
      ],
      "metadata": {
        "id": "8rq9oRBtXVJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def test(model, criterion, dataloader, device, epoch, logger, writer):\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    all_label = []\n",
        "    all_pred = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, data in enumerate(dataloader):\n",
        "            # 获取输入数据和标签\n",
        "            inputs, labels = data['data'].to(device), data['label'].to(device)\n",
        "            # 前向传播\n",
        "            outputs = model(inputs)\n",
        "            if isinstance(outputs, list):\n",
        "                outputs = outputs[0]\n",
        "            # 计算损失\n",
        "            loss = criterion(outputs, labels.squeeze())\n",
        "            losses.append(loss.item())\n",
        "            # 收集标签和预测结果\n",
        "            prediction = torch.max(outputs, 1)[1]\n",
        "            all_label.extend(labels.squeeze())\n",
        "            all_pred.extend(prediction)\n",
        "    # 计算平均损失和准确率\n",
        "    test_loss = sum(losses)/len(losses)\n",
        "    all_label = torch.stack(all_label, dim=0)\n",
        "    all_pred = torch.stack(all_pred, dim=0)\n",
        "    test_acc = accuracy_score(all_label.squeeze().cpu().data.squeeze().numpy(), all_pred.cpu().data.squeeze().numpy())\n",
        "    # 记录日志\n",
        "    writer.add_scalars('Loss', {'test': test_loss}, epoch+1)\n",
        "    writer.add_scalars('Accuracy', {'test': test_acc}, epoch+1)\n",
        "    logger.info(\"平均测试损失: {:.6f} | 准确率: {:.2f}%\".format(test_loss, test_acc*100))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    import os\n",
        "    import argparse\n",
        "    from torch.utils.data import DataLoader\n",
        "    import torchvision.transforms as transforms\n",
        "    #from dataset import CSL_Isolated\n",
        "    #from models.Conv3D import resnet18, resnet34, resnet50, r2plus1d_18\n",
        "    # 参数\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--data_path', default='/home/haodong/Data/CSL_Isolated/color_video_125000',\n",
        "        type=str, help='测试数据路径')\n",
        "    parser.add_argument('--label_path', default='/home/haodong/Data/CSL_Isolated/dictionary.txt',\n",
        "        type=str, help='标签文件路径')\n",
        "    parser.add_argument('--model', default='3dresnet18',\n",
        "        type=str, help='选择用于测试的模型')\n",
        "    parser.add_argument('--model_path', default='3dresnet18.pth',\n",
        "        type=str, help='模型状态字典路径')\n",
        "    parser.add_argument('--num_classes', default=500,\n",
        "        type=int, help='测试数据的类别数')\n",
        "    parser.add_argument('--batch_size', default=32,\n",
        "        type=int, help='测试批次大小')\n",
        "    parser.add_argument('--sample_size', default=128,\n",
        "        type=int, help='测试样本大小')\n",
        "    parser.add_argument('--sample_duration', default=16,\n",
        "        type=int, help='测试样本帧数')\n",
        "    parser.add_argument('--no_cuda', action='store_true',\n",
        "        help='如果为True，不使用CUDA')\n",
        "    parser.add_argument('--cuda_devices', default='2',\n",
        "        type=str, help='可见的CUDA设备')\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # 路径设置\n",
        "    data_path = args.data_path\n",
        "    label_path = args.label_path\n",
        "    model_path = args.model_path\n",
        "    # 使用特定的GPU\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"]=args.cuda_devices\n",
        "    # 设备设置\n",
        "    if torch.cuda.is_available() and not args.no_cuda:\n",
        "        device = torch.device(\"cuda\")\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "\n",
        "    # 超参数\n",
        "    num_classes = args.num_classes\n",
        "    batch_size = args.batch_size\n",
        "    sample_size = args.sample_size\n",
        "    sample_duration = args.sample_duration\n",
        "\n",
        "    # 开始测试\n",
        "    # 加载数据\n",
        "    transform = transforms.Compose([transforms.Resize([sample_size, sample_size]),\n",
        "                                    transforms.ToTensor(),\n",
        "                                    transforms.Normalize(mean=[0.5], std=[0.5])])\n",
        "    test_set = ASD_Isolated(data_path=data_path, label_path=label_path, frames=sample_duration,\n",
        "        num_classes=num_classes, train=False, transform=transform)\n",
        "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "\n",
        "    # 创建模型\n",
        "    if args.model == '3dresnet18':\n",
        "        model = resnet18(pretrained=True, progress=True, sample_size=sample_size,\n",
        "            sample_duration=sample_duration, num_classes=num_classes).to(device)\n",
        "    elif args.model == '3dresnet34':\n",
        "        model = resnet34(pretrained=True, progress=True, sample_size=sample_size,\n",
        "            sample_duration=sample_duration, num_classes=num_classes).to(device)\n",
        "    elif args.model == '3dresnet50':\n",
        "        model = resnet50(pretrained=True, progress=True, sample_size=sample_size,\n",
        "            sample_duration=sample_duration, num_classes=num_classes).to(device)\n",
        "    elif args.model == 'r2plus1d':\n",
        "        model = r2plus1d_18(pretrained=True, num_classes=num_classes).to(device)\n",
        "    # 并行运行模型\n",
        "    if torch.cuda.device_count() > 1:\n",
        "        model = nn.DataParallel(model)\n",
        "    # 加载模型\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "    # 测试模型\n",
        "    model.eval()\n",
        "    all_label = []\n",
        "    all_pred = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, data in enumerate(test_loader):\n",
        "            # 获取输入数据和标签\n",
        "            inputs, labels = data['data'].to(device), data['label'].to(device)\n",
        "            # 前向传播\n",
        "            outputs = model(inputs)\n",
        "            # 收集标签和预测结果\n",
        "            prediction = torch.max(outputs, 1)[1]\n",
        "            all_label.extend(labels.squeeze())\n",
        "            all_pred.extend(prediction)\n",
        "    # 计算平均损失和准确率\n",
        "    all_label = torch\n"
      ],
      "metadata": {
        "id": "7fGLxGidXUf4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}